{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19844352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e7d7c01",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this notebook, we will show how to evaluate the performance of a model on a test or validation set. \n",
    "This notebook assumes that the network has already been trained and saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b7f917",
   "metadata": {},
   "source": [
    "# Input\n",
    "1. data_dir: directory of the data. The datafile should be present in the data_dir.\n",
    "2. ckpt_dir: directory of the checkpoint. The checkpoint file and config should be present in the ckpt_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/ashesh.ashesh/paper_models/PaviaATN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2732797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/project2/nfschere/dzgong/SIM_three_channel_tiff'\n",
    "ckpt_dir = \"/home/dzgong/uSplit/train/2309/D16-M3-S0-L0/18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ec4422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzgong/mambaforge/envs/usplit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import ml_collections\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from usplit.training import create_dataset, create_model\n",
    "import matplotlib.pyplot as plt\n",
    "from usplit.core.loss_type import LossType\n",
    "from usplit.config_utils import load_config\n",
    "from usplit.analysis.lvae_utils import get_img_from_forward_output\n",
    "from usplit.analysis.plot_utils import clean_ax\n",
    "from usplit.core.data_type import DataType\n",
    "from usplit.core.psnr import PSNR\n",
    "from usplit.analysis.plot_utils import get_k_largest_indices,plot_imgs_from_idx\n",
    "from usplit.core.psnr import PSNR, RangeInvariantPsnr\n",
    "from usplit.core.data_split_type import DataSplitType\n",
    "from usplit.analysis.stitch_prediction import stitch_predictions\n",
    "from usplit.analysis.mmse_prediction import get_dset_predictions\n",
    "\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f6ee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7232e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = int(ckpt_dir.split('/')[-2].split('-')[0][1:])\n",
    "DataType.name(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237569",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_size_for_grid_centers = 32\n",
    "mmse_count = 1\n",
    "custom_image_size = 64\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "use_deterministic_grid = None\n",
    "threshold = None # 0.02\n",
    "compute_kl_loss = False\n",
    "evaluate_train = False# inspect training performance\n",
    "eval_datasplit_type = DataSplitType.Test\n",
    "val_repeat_factor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3360bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_checkpoint(ckpt_dir):\n",
    "    output = []\n",
    "    for filename in glob.glob(ckpt_dir + \"/*_best.ckpt\"):\n",
    "        output.append(filename)\n",
    "    assert len(output) == 1, '\\n'.join(output)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usplit.core.model_type import ModelType\n",
    "config = load_config(ckpt_dir)\n",
    "config = ml_collections.ConfigDict(config)\n",
    "old_image_size = None\n",
    "with config.unlocked():\n",
    "    if 'test_fraction' not in config.training:\n",
    "        config.training.test_fraction =0.0\n",
    "        \n",
    "    if 'datadir' not in config:\n",
    "        config.datadir = ''\n",
    "    if 'encoder' not in config.model:\n",
    "        config.model.encoder = ml_collections.ConfigDict()\n",
    "        assert 'decoder' not in config.model\n",
    "        config.model.decoder = ml_collections.ConfigDict()\n",
    "    \n",
    "        config.model.encoder.dropout = config.model.dropout\n",
    "        config.model.decoder.dropout = config.model.dropout\n",
    "        config.model.encoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.decoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.encoder.n_filters = config.model.n_filters\n",
    "        config.model.decoder.n_filters = config.model.n_filters\n",
    "        \n",
    "    if 'multiscale_retain_spatial_dims' not in config.model.decoder:\n",
    "        config.model.decoder.multiscale_retain_spatial_dims = False\n",
    "        \n",
    "    if 'res_block_kernel' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_kernel = 3\n",
    "        assert 'res_block_kernel' not in config.model.decoder\n",
    "        config.model.decoder.res_block_kernel = 3\n",
    "    \n",
    "    if 'res_block_skip_padding' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_skip_padding = False\n",
    "        assert 'res_block_skip_padding' not in config.model.decoder\n",
    "        config.model.decoder.res_block_skip_padding = False\n",
    "    \n",
    "    if config.data.data_type == DataType.CustomSinosoid:\n",
    "        if 'max_vshift_factor' not in config.data:\n",
    "            config.data.max_vshift_factor = config.data.max_shift_factor\n",
    "            config.data.max_hshift_factor = 0\n",
    "        if 'encourage_non_overlap_single_channel' not in config.data:\n",
    "            config.data.encourage_non_overlap_single_channel = False\n",
    "            \n",
    "    if 'skip_bottom_layers_count' in config.model:\n",
    "        config.model.skip_bottom_layers_count = 0\n",
    "        \n",
    "    if 'logvar_lowerbound' not in config.model:\n",
    "        config.model.logvar_lowerbound = None\n",
    "    if 'train_aug_rotate' not in config.data:\n",
    "        config.data.train_aug_rotate = False\n",
    "    if 'multiscale_lowres_separate_branch' not in config.model:\n",
    "        config.model.multiscale_lowres_separate_branch = False\n",
    "    if 'multiscale_retain_spatial_dims' not in config.model:\n",
    "        config.model.multiscale_retain_spatial_dims = False\n",
    "    config.data.train_aug_rotate=False\n",
    "    \n",
    "    if 'randomized_channels' not in config.data:\n",
    "        config.data.randomized_channels = False\n",
    "        \n",
    "    if 'predict_logvar' not in config.model:\n",
    "        config.model.predict_logvar=None\n",
    "    \n",
    "    if 'batchnorm' in config.model and 'batchnorm' not in config.model.encoder:\n",
    "        assert 'batchnorm' not in config.model.decoder\n",
    "        config.model.decoder.batchnorm = config.model.batchnorm\n",
    "        config.model.encoder.batchnorm = config.model.batchnorm\n",
    "    if 'conv2d_bias' not in config.model.decoder:\n",
    "        config.model.decoder.conv2d_bias = True\n",
    "        \n",
    "    if config.data.data_type in [DataType.OptiMEM100_014, DataType.CustomSinosoid, DataType.SeparateTiffData,\n",
    "                                DataType.CustomSinosoidThreeCurve, DataType.DZGongSIM]:\n",
    "        if custom_image_size is not None:\n",
    "            old_image_size = config.data.image_size\n",
    "            config.data.image_size = custom_image_size\n",
    "        if image_size_for_grid_centers is not None:\n",
    "            old_grid_size = config.data.get('grid_size', \"grid_size not present\")\n",
    "            config.data.grid_size = image_size_for_grid_centers\n",
    "            config.data.val_grid_size = image_size_for_grid_centers\n",
    "\n",
    "        if use_deterministic_grid is not None:\n",
    "            config.data.deterministic_grid = use_deterministic_grid\n",
    "        if threshold is not None:\n",
    "            config.data.threshold = threshold\n",
    "        if val_repeat_factor is not None:\n",
    "            config.training.val_repeat_factor = val_repeat_factor\n",
    "        config.model.mode_pred = not compute_kl_loss\n",
    "\n",
    "    config.model.skip_nboundary_pixels_from_loss = None\n",
    "    if config.model.model_type == ModelType.UNet and 'n_levels' not in config.model:\n",
    "        config.model.n_levels = 4\n",
    "    \n",
    "    if config.model.model_type == ModelType.UNet and 'init_channel_count' not in config.model:\n",
    "        config.model.init_channel_count = 64\n",
    "    \n",
    "    if 'skip_receptive_field_loss_tokens' not in config.loss:\n",
    "        config.loss.skip_receptive_field_loss_tokens = []\n",
    "    \n",
    "    if 'lowres_merge_type' not in config.model.encoder:\n",
    "        config.model.encoder.lowres_merge_type = 0\n",
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cfc106",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usplit.data_loader.multi_channel_determ_tiff_dloader import MultiChDeterministicTiffDloader\n",
    "from usplit.data_loader.lc_tiff_dloader import MultiScaleTiffDloader\n",
    "from usplit.core.data_split_type import DataSplitType\n",
    "from usplit.data_loader.patch_index_manager import GridAlignement\n",
    "\n",
    "padding_kwargs = {\n",
    "    'mode':config.data.get('padding_mode','constant'),\n",
    "}\n",
    "\n",
    "if padding_kwargs['mode'] == 'constant':\n",
    "    padding_kwargs['constant_values'] = config.data.get('padding_value',0)\n",
    "\n",
    "dloader_kwargs = {'overlapping_padding_kwargs':padding_kwargs}\n",
    "if 'multiscale_lowres_count' in config.data and config.data.multiscale_lowres_count is not None:\n",
    "    data_class = MultiScaleTiffDloader\n",
    "    dloader_kwargs['num_scales'] = config.data.multiscale_lowres_count\n",
    "    dloader_kwargs['padding_kwargs'] = padding_kwargs\n",
    "else:\n",
    "    data_class = MultiChDeterministicTiffDloader\n",
    "\n",
    "if config.data.data_type in [DataType.CustomSinosoid, DataType.CustomSinosoidThreeCurve, \n",
    "                             DataType.SeparateTiffData,\n",
    "                            ]:\n",
    "    datapath = data_dir\n",
    "elif config.data.data_type == DataType.OptiMEM100_014:\n",
    "    datapath = os.path.join(data_dir, 'OptiMEM100x014.tif')\n",
    "elif config.data.data_type == DataType.DZGongSIM:\n",
    "    datapath = os.path.join(data_dir, 'SIM201-240.tif')\n",
    "    # datapath = os.path.join(data_dir, 'SIM-10frames-3channel.tif')\n",
    "else:\n",
    "    raise NotImplementedError(config.data.data_type)\n",
    "\n",
    "normalized_input = config.data.normalized_input\n",
    "use_one_mu_std = config.data.use_one_mu_std\n",
    "train_aug_rotate = config.data.train_aug_rotate\n",
    "enable_random_cropping = False\n",
    "grid_alignment = GridAlignement.Center\n",
    "print(data_class)\n",
    "\n",
    "train_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=DataSplitType.Train,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=train_aug_rotate,\n",
    "                enable_random_cropping=enable_random_cropping,\n",
    "                grid_alignment=grid_alignment,\n",
    "                **dloader_kwargs)\n",
    "import gc\n",
    "gc.collect()\n",
    "max_val = train_dset.get_max_val()\n",
    "\n",
    "val_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=eval_datasplit_type,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=False,  # No rotation aug on validation\n",
    "                enable_random_cropping=False,\n",
    "                # No random cropping on validation. Validation is evaluated on determistic grids\n",
    "                grid_alignment=grid_alignment,\n",
    "                max_val=max_val,\n",
    "                **dloader_kwargs\n",
    "                \n",
    "            )\n",
    "\n",
    "# For normalizing, we should be using the training data's mean and std.\n",
    "mean_val, std_val = train_dset.compute_mean_std()\n",
    "train_dset.set_mean_std(mean_val, std_val)\n",
    "val_dset.set_mean_std(mean_val, std_val)\n",
    "\n",
    "\n",
    "if evaluate_train:\n",
    "    val_dset = train_dset\n",
    "data_mean, data_std = train_dset.get_mean_std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c69e23b",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with config.unlocked():\n",
    "    if old_image_size is not None:\n",
    "        config.data.image_size = old_image_size\n",
    "\n",
    "if config.data.target_separate_normalization is True:\n",
    "    mean_fr_model, std_fr_model = train_dset.compute_individual_mean_std()\n",
    "else:\n",
    "    mean_fr_model, std_fr_model = train_dset.get_mean_std()\n",
    "\n",
    "\n",
    "model = create_model(config, mean_fr_model,std_fr_model)\n",
    "\n",
    "ckpt_fpath = get_best_checkpoint(ckpt_dir)\n",
    "checkpoint = torch.load(ckpt_fpath)\n",
    "\n",
    "_ = model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "_= model.cuda()\n",
    "\n",
    "model.set_params_to_same_device_as(torch.Tensor(1).cuda())\n",
    "\n",
    "print('Loading from epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model has {count_parameters(model)/1000_000:.3f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69da4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.multiscale_lowres_count is not None and custom_image_size is not None:\n",
    "    model.reset_for_different_output_size(custom_image_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d23d6c0",
   "metadata": {},
   "source": [
    "## Looking at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "if inp_tmp.ndim == 2:\n",
    "    inp_tmp = inp_tmp[None,...]\n",
    "\n",
    "ncols = max(len(inp_tmp),3)\n",
    "nrows = 2\n",
    "_,ax = plt.subplots(figsize=(4*ncols,4*nrows),ncols=ncols,nrows=nrows)\n",
    "for i in range(len(inp_tmp)):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "\n",
    "ax[1,0].imshow(tar_tmp[0])\n",
    "ax[1,1].imshow(tar_tmp[1])\n",
    "\n",
    "ax[0,0].set_ylabel('Input')\n",
    "ax[1,0].set_ylabel('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, tar = val_dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1691515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tiled, rec_loss, logvar, patch_psnr_tuple = get_dset_predictions(model, val_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=mmse_count,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b693a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = np.where(logvar.squeeze() < -6)[0]\n",
    "if len(idx_list) > 0:\n",
    "    plt.imshow(val_dset[idx_list[0]][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stitch_predictions(pred_tiled,val_dset, smoothening_pixelcount=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ignored_pixels():\n",
    "    ignored_pixels = 1\n",
    "    while(pred[0,-ignored_pixels:,-ignored_pixels:,].std() ==0):\n",
    "        ignored_pixels+=1\n",
    "    ignored_pixels-=1\n",
    "    print(f'In {pred.shape}, last {ignored_pixels} many rows and columns are all zero.')\n",
    "    return ignored_pixels\n",
    "\n",
    "ignored_pixels_in_data = print_ignored_pixels()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8474735",
   "metadata": {},
   "source": [
    "## Ignore the pixels which are present in the last few rows and columns. \n",
    "1. They don't come in the batches. So, in prediction, they are simply zeros. So they are being are ignored right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_pixels_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadedfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_last_pixels_count = 32 if config.data.data_type == DataType.OptiMEM100_014 else ignored_pixels_in_data\n",
    "\n",
    "assert ignored_pixels_in_data <= ignore_last_pixels_count, f'Set ignore_last_pixels_count={ignored_pixels_in_data}'\n",
    "print(ignore_last_pixels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = val_dset._data\n",
    "def ignore_pixels(arr):\n",
    "    if ignore_last_pixels_count:\n",
    "        arr = arr[:,:-ignore_last_pixels_count,:-ignore_last_pixels_count]\n",
    "    return arr\n",
    "\n",
    "pred = ignore_pixels(pred)\n",
    "tar = ignore_pixels(tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "def _avg_psnr(target, prediction, psnr_fn):\n",
    "    output = np.mean([psnr_fn(target[i:i + 1], prediction[i:i + 1]).item() for i in range(len(prediction))])\n",
    "    return round(output, 2)\n",
    "\n",
    "\n",
    "def avg_range_inv_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, RangeInvariantPsnr)\n",
    "\n",
    "\n",
    "def avg_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, PSNR)\n",
    "\n",
    "\n",
    "def compute_masked_psnr(mask, tar1, tar2, pred1, pred2):\n",
    "    mask = mask.astype(bool)\n",
    "    mask = mask[..., 0]\n",
    "    tmp_tar1 = tar1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_pred1 = pred1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_tar2 = tar2[mask].reshape((len(tar2), -1, 1))\n",
    "    tmp_pred2 = pred2[mask].reshape((len(tar2), -1, 1))\n",
    "    psnr1 = avg_range_inv_psnr(tmp_tar1, tmp_pred1)\n",
    "    psnr2 = avg_range_inv_psnr(tmp_tar2, tmp_pred2)\n",
    "    return psnr1, psnr2\n",
    "\n",
    "def avg_ssim(target, prediction):\n",
    "    ssim = [structural_similarity(target[i],prediction[i], data_range=(target[i].max() - target[i].min())) for i in range(len(target))]\n",
    "    return np.mean(ssim),np.std(ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_mean, sep_std = model.data_mean, model.data_std\n",
    "if isinstance(sep_mean, dict):\n",
    "    sep_mean = sep_mean['target']\n",
    "    sep_std = sep_std['target']\n",
    "    \n",
    "sep_mean = sep_mean.squeeze()[None,None,None]\n",
    "sep_std = sep_std.squeeze()[None,None,None]\n",
    "\n",
    "tar_normalized = (tar - sep_mean.cpu().numpy())/sep_std.cpu().numpy()\n",
    "tar1 =tar_normalized[...,0]\n",
    "tar2 =tar_normalized[...,1]\n",
    "tar3 =tar_normalized[...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24708c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(12,12),ncols=3,nrows=2)\n",
    "idx = np.random.randint(len(pred))\n",
    "print(idx)\n",
    "ax[0,0].imshow(pred[idx,:,:,0])\n",
    "ax[0,1].imshow(pred[idx,:,:,1])\n",
    "ax[0,2].imshow(pred[idx,:,:,2])\n",
    "ax[1,0].imshow(tar1[idx,:,:])\n",
    "ax[1,1].imshow(tar2[idx,:,:])\n",
    "ax[1,2].imshow(tar3[idx,:,:])\n",
    "# ax[1,3].imshow(tar1[idx,:,:]+tar2[idx,:,:]+tar3[idx,:,:])\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar[...,2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615853da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1, pred2, pred3 = pred[...,0].astype(np.float32), pred[...,1].astype(np.float32), pred[...,2].astype(np.float32)\n",
    "rmse1 =np.sqrt(((pred1 - tar1)**2).reshape(len(pred1),-1).mean(axis=1))\n",
    "rmse2 =np.sqrt(((pred2 - tar2)**2).reshape(len(pred2),-1).mean(axis=1)) \n",
    "rmse3 =np.sqrt(((pred3 - tar3)**2).reshape(len(pred3),-1).mean(axis=1)) \n",
    "\n",
    "rmse = (rmse1 + rmse2+rmse3)/2\n",
    "rmse = np.round(rmse,3)\n",
    "\n",
    "psnr1 = avg_psnr(tar1, pred1) \n",
    "psnr2 = avg_psnr(tar2, pred2)\n",
    "psnr3 = avg_psnr(tar3, pred3)\n",
    "rinv_psnr1 = avg_range_inv_psnr(tar1, pred1)\n",
    "rinv_psnr2 = avg_range_inv_psnr(tar2, pred2)\n",
    "rinv_psnr3 = avg_range_inv_psnr(tar3, pred3)\n",
    "ssim1_mean, ssim1_std = avg_ssim(tar1, pred1)\n",
    "ssim2_mean, ssim2_std = avg_ssim(tar2, pred2)\n",
    "ssim3_mean, ssim3_std = avg_ssim(tar3, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1, pred2 = pred[...,0].astype(np.float32), pred[...,1].astype(np.float32)\n",
    "# rmse1 =np.sqrt(((pred1 - tar1)**2).reshape(len(pred1),-1).mean(axis=1))\n",
    "# rmse2 =np.sqrt(((pred2 - tar2)**2).reshape(len(pred2),-1).mean(axis=1)) \n",
    "\n",
    "# rmse = (rmse1 + rmse2)/2\n",
    "# rmse = np.round(rmse,3)\n",
    "# psnr1 = avg_psnr(tar1, pred1) \n",
    "# psnr2 = avg_psnr(tar2, pred2)\n",
    "# rinv_psnr1 = avg_range_inv_psnr(tar1, pred1)\n",
    "# rinv_psnr2 = avg_range_inv_psnr(tar2, pred2)\n",
    "# ssim1_mean, ssim1_std = avg_ssim(tar1, pred1)\n",
    "# ssim2_mean, ssim2_std = avg_ssim(tar2, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87868b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{DataSplitType.name(eval_datasplit_type)}_P{custom_image_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignore_last_pixels_count}')\n",
    "print('Rec Loss',np.round(rec_loss.mean(),3) )\n",
    "print('RMSE', np.mean(rmse1).round(3), np.mean(rmse2).round(3), np.mean(rmse3).round(3),np.mean(rmse).round(3))\n",
    "print('PSNR', psnr1, psnr2, psnr3)\n",
    "print('[Paper] RangeInv PSNR',rinv_psnr1, rinv_psnr2, rinv_psnr3)\n",
    "print('[Paper] SSIM',round(ssim1_mean,3), round(ssim2_mean,3), round(ssim3_mean,3),'±',round((ssim1_std + ssim2_std+ssim3_std)/3,4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0035b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "\n",
    "\n",
    "imsave('/home/daozhengg/data/SIM_all/result/10frames_pred_1_0', pred[0,:,:,2], plugin='tifffile')\n",
    "# imsave('/home/daozhengg/data/SIM_all/result/tar_1', pred, plugin='tifffile')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75ba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Disentangle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b857d1bba0930ca022eebf5eaf82be7e85c90a51a5292e3d1e5b098bb58d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
